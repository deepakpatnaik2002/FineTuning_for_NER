{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Getting Stats for ChatGPT modelled labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# make sure the files(colab_maual.txt and chatGPR_aligned.txt) are present befor executing this cell\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score,classification_report\n",
        "def calculate_metrics(predicted_labels, ground_truth_labels):\n",
        "    precision = precision_score(ground_truth_labels, predicted_labels, average='macro')\n",
        "    recall = recall_score(ground_truth_labels, predicted_labels, average='macro')\n",
        "    f1_sco = f1_score(ground_truth_labels, predicted_labels, average='macro')\n",
        "    \n",
        "    return precision, recall, f1_sco\n",
        "\n",
        "final_l1,final_l2=[],[]\n",
        "# Iterate over both files simultaneously\n",
        "with open('colab_maual.txt', 'r', encoding='utf-8') as file1, open('chatGPT_aligned.txt', 'r', encoding='utf-8') as file2:\n",
        "    for line1, line2 in zip((file1), (file2)):\n",
        "      final_l1.extend(eval(line1)[\"ner_tags\"])\n",
        "      final_l2.extend(eval(line2)[\"ner_tags\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For label 'I-LOC':\n",
            "Precision: 0.00, Recall: 0.00, F1-score: 0.00\n",
            "\n",
            "For label 'B-PER':\n",
            "Precision: 0.00, Recall: 0.00, F1-score: 0.00\n",
            "\n",
            "For label 'I-TIME':\n",
            "Precision: 0.00, Recall: 0.00, F1-score: 0.00\n",
            "\n",
            "For label 'I-MISC':\n",
            "Precision: 0.00, Recall: 0.00, F1-score: 0.00\n",
            "\n",
            "For label 'B-MISC':\n",
            "Precision: 0.00, Recall: 0.00, F1-score: 0.00\n",
            "\n",
            "For label 'B-ORG':\n",
            "Precision: 1.00, Recall: 0.20, F1-score: 0.33\n",
            "\n",
            "For label 'O':\n",
            "Precision: 0.86, Recall: 0.98, F1-score: 0.92\n",
            "\n",
            "For label 'O':\n",
            "Precision: 0.86, Recall: 0.98, F1-score: 0.92\n",
            "\n",
            "For label 'B-LOC':\n",
            "Precision: 0.00, Recall: 0.00, F1-score: 0.00\n",
            "\n",
            "For label 'B-TIME':\n",
            "Precision: 0.00, Recall: 0.00, F1-score: 0.00\n",
            "\n",
            "For label 'I-PER':\n",
            "Precision: 0.50, Recall: 0.09, F1-score: 0.15\n",
            "\n",
            "For label 'B-AGE':\n",
            "Precision: 0.00, Recall: 0.00, F1-score: 0.00\n",
            "\n",
            "For label 'I-ORG':\n",
            "Precision: 1.00, Recall: 0.25, F1-score: 0.40\n",
            "\n",
            "Macro_precision:  0.3245192307692308\n",
            "Macro_Recall:  0.19300768813691277\n",
            "Macro_F1-score:  0.20940091727858176\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def convert_to_binary_classification(y_true, y_pred, label):\n",
        "    # Convert the labels to binary classification (positive class: label, negative class: not label)\n",
        "    y_true_binary = [1 if true_label == label else 0 for true_label in y_true]\n",
        "    y_pred_binary = [1 if pred_label == label else 0 for pred_label in y_pred]\n",
        "    return y_true_binary, y_pred_binary\n",
        "\n",
        "def calculate_binary_classification_metrics(y_true_binary, y_pred_binary):\n",
        "    # Calculate precision, recall, and F1-score for binary classification\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_true_binary, y_pred_binary, average='binary')\n",
        "    return precision, recall, f1_score\n",
        "\n",
        "def convert_to_uniclass_classification(y_true, y_pred):\n",
        "    unique_labels = set(y_true) | set(y_pred)\n",
        "    psum,rsum,f1sum=0,0,0\n",
        "    for label in unique_labels:\n",
        "        if label==\"OO\": label=\"O\"\n",
        "        y_true_binary, y_pred_binary = convert_to_binary_classification(y_true, y_pred, label)\n",
        "        precision, recall, f1_score = calculate_binary_classification_metrics(y_true_binary, y_pred_binary)\n",
        "        print(f\"For label '{label}':\")\n",
        "        print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1_score:.2f}\\n\")\n",
        "        psum+=precision\n",
        "        rsum+=recall\n",
        "        f1sum+=f1_score\n",
        "    print(\"Macro_precision: \",psum/len(unique_labels))\n",
        "    print(\"Macro_Recall: \",rsum/len(unique_labels))\n",
        "    print(\"Macro_F1-score: \",f1sum/len(unique_labels))\n",
        "convert_to_uniclass_classification(final_l1, final_l2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
